{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064005b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pac.net2 import *\n",
    "from pac.data_gen2 import DataGen2\n",
    "from graphing import plot_heatmap\n",
    "from pac.data_loss import *\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d265846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([500])) that is different to the input size (torch.Size([500, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch --- loss: 0 34712960.0\n",
      "i_epoch --- loss: 100 32062.845703125\n",
      "i_epoch --- loss: 200 9352.06640625\n",
      "i_epoch --- loss: 300 4277.62255859375\n",
      "i_epoch --- loss: 400 2583.423828125\n",
      "i_epoch --- loss: 500 1793.2589111328125\n",
      "i_epoch --- loss: 600 1295.80126953125\n",
      "i_epoch --- loss: 700 979.2694091796875\n",
      "i_epoch --- loss: 800 877.6751098632812\n",
      "i_epoch --- loss: 900 648.3428955078125\n",
      "RunTime: 0h-5m--19s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    starttime = datetime.now()\n",
    "    # 输出数据的初始化\n",
    "    zs = 0.1\n",
    "    ze = 2\n",
    "    ts = 1\n",
    "    te = 3000\n",
    "    zsteps = 10\n",
    "    tsteps = 300\n",
    "    it_batch_size = 2000\n",
    "    bd_batch_size = 500\n",
    "    init_batch_size = 500\n",
    "    ws = 0.001\n",
    "    ds = 0.0002\n",
    "    p = 0.0001\n",
    "    # 生成数据\n",
    "\n",
    "    data = DataGen2(zs, ze, ts, te, zsteps=zsteps, tsteps=tsteps, ws=ws, ds=ds, p=p)\n",
    "    dim_in = 2\n",
    "    dim_out = 1\n",
    "    # hidden_list = (50, 40, 30, 30, 20)\n",
    "    hidden_list = (20, 10, 8, 8, 5)\n",
    "    model_name = 'DNN'\n",
    "    init_lr = 0.01\n",
    "    max_it = 1000\n",
    "\n",
    "    # 神经网络各组分的初始化\n",
    "    # model = DNN1(indim=dim_in, outdim=dim_out, hidden_units=hidden_list, name2Model=model_name, actName2in='tanh',\n",
    "    #                 actName='tanh')\n",
    "    # model = Pure_DenseNet(indim=dim_in, outdim=dim_out, hidden_units=hidden_list, name2Model=model_name, actName2in='tanh',\n",
    "    #                 actName='tanh')\n",
    "    # model = PDE_DNN(dim_in=dim_in, dim_out=dim_out, hidden_layers=hidden_list, name2Model=model_name, actName_in='tanh',\n",
    "    # actName_hidden='tanh', ws=ws, ds=ds)\n",
    "    model = PDE_DNN(dim_in=dim_in, dim_out=dim_out, hidden_layers=hidden_list, name2Model=model_name, actName_in='sin',\n",
    "                    actName_hidden='sin', ws=ws, ds=ds)\n",
    "    params2Net = model.DNN1.parameters()\n",
    "    optimizer = torch.optim.Adam(params2Net, lr=init_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.995)\n",
    "    arr2epoch = []\n",
    "    arr2loss = []\n",
    "    arr2lr = []\n",
    "    predi = []\n",
    "    loss_append = []\n",
    "    lr_append = []\n",
    "    init_netloss = []\n",
    "    it_netloss = []\n",
    "    bd_netloss = []\n",
    "    for i_epoch in range(max_it):\n",
    "\n",
    "        # 每次迭代都随机生成数据\n",
    "        it_inputs, it_labels = data.gen_inter_m(it_batch_size)\n",
    "        bd_inputs, bd_labels = data.gen_bound(bd_batch_size)\n",
    "        init_inputs, init_labels = data.gen_init(init_batch_size)\n",
    "        loss1 = model.loss_init_net(init_inputs, init_labels)\n",
    "        loss2 = model.loss_it_pde(it_inputs, it_labels)\n",
    "        loss3 = model.loss_bd_net(bd_inputs, bd_labels)\n",
    "        # ds是常数损失函数修改了 内部点的pde损失\n",
    "        # loss = 1000 * model.loss_it_net(it_inputs, it_labels) + model.loss_it_pde(it_inputs, it_labels) \\\n",
    "        # + 1000 * model.loss_bd_net(bd_inputs, bd_labels)\n",
    "        loss = 100 * loss1 + loss2 + 200*loss3\n",
    "        # loss = 1000 * model.loss_init_net(init_inputs, init_labels) + model.loss_it_pde(it_inputs, it_labels) \\\n",
    "        # + 1000 * model.loss_bd_net(bd_inputs, bd_labels)\n",
    "        loss_append.append(loss.item())\n",
    "        init_netloss.append(loss1.detach().numpy())\n",
    "        it_netloss.append(loss2.detach().numpy())\n",
    "        bd_netloss.append(loss3.detach().numpy())\n",
    "        lr_append.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        prediction = model.evaluate(it_inputs)\n",
    "        predi.append(prediction)\n",
    "        optimizer.zero_grad()  # 求导前先清零, 只要在下一次求导前清零即可\n",
    "        loss.backward()  # 求偏导\n",
    "        optimizer.step()  # 更新参数\n",
    "        scheduler.step()\n",
    "\n",
    "        if i_epoch % 100 == 0:\n",
    "            print('i_epoch --- loss:', i_epoch, loss.item())\n",
    "            # print(\"第%d个epoch的学习率：%f\" % (i_epoch, optimizer.param_groups[0]['lr']))\n",
    "            arr2loss.append(loss.item())\n",
    "            arr2lr.append(optimizer.param_groups[0]['lr'])\n",
    "    endtime = datetime.now()\n",
    "    print(\"RunTime: {}h-{}m-{}s\".format(endtime.hour - starttime.hour, endtime.minute - starttime.minute,\n",
    "                                        endtime.second - starttime.second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # -------------show loss----------#\n",
    "    logloss = np.log(loss_append)\n",
    "    plt.title(\"log(sum loss) trend\")\n",
    "    plt.plot(logloss, color='b', label='Label')\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"log(loss)\")\n",
    "    plt.show()\n",
    "    plot_heatmap(data, model, max_it)\n",
    "    log_0 = np.log(it_netloss)\n",
    "    log_1 = np.log(bd_netloss)\n",
    "    log_2 = np.log(init_netloss)\n",
    "    log_3 = np.log(loss_append)\n",
    "    plt.title(\"loss trend\")\n",
    "    plt.plot(log_0, color='r')\n",
    "    plt.plot(log_1, color='b')\n",
    "    plt.plot(log_2, color='y')\n",
    "    plt.plot(log_3, color='g')\n",
    "    plt.legend(('interior_net loss', 'bd_net loss', 'init_net loss', 'total loss'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
